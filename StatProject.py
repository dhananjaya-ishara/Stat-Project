# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yX6Q6WlmRo3JoTmthvDcYMMA1knWNipl
"""

from scipy import stats
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

#CREATING THE DATASET FOR HYPOTHESIS
import pandas as pd
import numpy as np

# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic dataset
n = 500  # Number of individuals
age = np.random.randint(20, 80, size=n)  # Age between 20 and 80
bmi = np.random.normal(loc=25, scale=5, size=n)  # BMI with mean 25 and std 5
blood_pressure = np.random.normal(loc=120, scale=15, size=n)  # Systolic BP with mean 120
diabetes_status = np.random.choice([0, 1], size=n, p=[0.8, 0.2])  # 20% diabetic, 80% non-diabetic
smoker_status = np.random.choice([0, 1], size=n, p=[0.7, 0.3])  # 30% smokers

# Create DataFrame
df = pd.DataFrame({
    "Age": age,
    "BMI": bmi,
    "Blood_Pressure": blood_pressure,
    "Diabetes": diabetes_status,
    "Smoker": smoker_status
})

# Save to CSV
df.to_csv("health_data.csv", index=False)

print("Dataset generated and saved as 'health_data.csv'.")

# Separate blood pressure by diabetes status
bp_diabetic = df[df['Diabetes'] == 1]['Blood_Pressure']
bp_non_diabetic = df[df['Diabetes'] == 0]['Blood_Pressure']

# 1. Check assumptions
print(" Assumption Checks ")
# Normality
_, p_diabetic = stats.shapiro(bp_diabetic)
_, p_non_diabetic = stats.shapiro(bp_non_diabetic)
print(f"Normality p-values: Diabetic={p_diabetic:.3f}, Non-diabetic={p_non_diabetic:.3f}")

# Equal variance
_, p_levene = stats.levene(bp_diabetic, bp_non_diabetic)
print(f"Equal variance p-value (Levene's): {p_levene:.3f}")

# 2. Visualize distributions
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
sns.boxplot(x='Diabetes', y='Blood_Pressure', data=df)
plt.title("Blood Pressure by Diabetes Status")

plt.subplot(1, 2, 2)
sns.histplot(bp_diabetic, color='red', label='Diabetic', kde=True)
sns.histplot(bp_non_diabetic, color='blue', label='Non-diabetic', kde=True)
plt.legend()
plt.title("Distribution Plot")
plt.tight_layout()
plt.show()

# 3. Perform independent t-test
t_stat, p_value = stats.ttest_ind(bp_diabetic, bp_non_diabetic, equal_var=True)
print("\n=== Hypothesis Test Results ===")
print(f"Sample sizes: Diabetic={len(bp_diabetic)}, Non-diabetic={len(bp_non_diabetic)}")
print(f"Mean BP: Diabetic={bp_diabetic.mean():.1f}, Non-diabetic={bp_non_diabetic.mean():.1f}")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_value:.4f}")

# 4. Calculate effect size
pooled_std = np.sqrt((bp_diabetic.std()**2 + bp_non_diabetic.std()**2)/2)
cohens_d = (bp_diabetic.mean() - bp_non_diabetic.mean()) / pooled_std
print(f"Cohen's d: {cohens_d:.4f}")

# 5. Confidence interval for mean difference
diff_mean = bp_diabetic.mean() - bp_non_diabetic.mean()
diff_se = pooled_std * np.sqrt(1/len(bp_diabetic) + 1/len(bp_non_diabetic))
ci_low = diff_mean - 1.96 * diff_se
ci_high = diff_mean + 1.96 * diff_se
print(f"95% CI for mean difference: ({ci_low:.2f}, {ci_high:.2f})")

# 6. Conclusion
alpha = 0.05
if p_value < alpha:
    print("\nConclusion: Reject H₀ - Diabetics have significantly different blood pressure.")
else:
    print("\nConclusion: Fail to reject H₀ - No significant difference in blood pressure.")

import pandas as pd
import numpy as np

# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic dataset
n = 250  # Number of employees per group
work_hours_A = np.random.uniform(6, 8, size=n)  # Group A: 6-8 hours/day
work_hours_B = np.random.uniform(9, 12, size=n)  # Group B: 9-12 hours/day
performance_A = np.random.normal(loc=70, scale=10, size=n)  # Group A: Mean 70, Std Dev 10
performance_B = np.random.normal(loc=85, scale=12, size=n)  # Group B: Mean 85, Std Dev 12

# Create DataFrame
df = pd.DataFrame({
    "Employee_ID": range(1, n*2+1),
    "Group": ["A"]*n + ["B"]*n,
    "Work_Hours": np.concatenate([work_hours_A, work_hours_B]),
    "Performance_Score": np.concatenate([performance_A, performance_B])
})

# Save to CSV
df.to_csv("employee_performance.csv", index=False)

print("Dataset generated and saved as 'employee_performance.csv'.")

#CLEANING THE DATASET
# Load dataset
df = pd.read_csv("/content/employee_performance.csv")

### 1. Check for Missing Values ###
missing_values = df.isnull().sum()
print("Missing values per column:\n", missing_values)

# If missing values exist, fill with median (for robustness)
df.fillna(df.select_dtypes(include=['number']).median(), inplace=True)


### 2. Check for Outliers (Using Boxplots) ###
import seaborn as sns
import matplotlib.pyplot as plt

sns.boxplot(x=df["Performance_Score"])
plt.title("Outlier Detection - Performance Score")
plt.show()

# Remove extreme outliers (beyond 1st & 99th percentile)
df = df[df["Performance_Score"].between(df["Performance_Score"].quantile(0.01), df["Performance_Score"].quantile(0.99))]

### 3. Check for Duplicate Entries ###
duplicates = df.duplicated().sum()
print("Duplicate entries found:", duplicates)

# Remove duplicates
df.drop_duplicates(inplace=True)

import scipy.stats as stats

# Separate scores for Group A and Group B
scores_A = df[df["Group"] == "A"]["Performance_Score"]
scores_B = df[df["Group"] == "B"]["Performance_Score"]

# Perform Mann-Whitney U Test
u_stat, p_value = stats.mannwhitneyu(scores_A, scores_B, alternative="two-sided")

print(f"Mann-Whitney U Test for Performance Scores:")
print(f"U-statistic: {u_stat:.4f}")
print(f"P-value: {p_value:.4f}")

# Decision based on p-value
if p_value < 0.05:
    print("Conclusion: Reject H₀ - Performance score distributions are significantly different.")
else:
    print("Conclusion: Fail to reject H₀ - No significant difference in performance scores.")

#LINEAR REGRESSION ANALYSIS
import pandas as pd
import numpy as np

# Set random seed
np.random.seed(42)

# Generate synthetic dataset
n = 500
size_sqft = np.random.randint(800, 4000, size=n)
bedrooms = np.random.randint(1, 6, size=n)
bathrooms = np.random.randint(1, 4, size=n)
garage = np.random.choice([0, 1], size=n, p=[0.4, 0.6])
distance_to_city = np.random.uniform(1, 50, size=n)
price = size_sqft * np.random.uniform(0.08, 0.12, size=n) + (bedrooms * 10) + (bathrooms * 15) - (distance_to_city * 5) + (garage * 20)

# Create DataFrame
df1 = pd.DataFrame({
    "House_ID": range(1, n+1),
    "Size_sqft": size_sqft,
    "Bedrooms": bedrooms,
    "Bathrooms": bathrooms,
    "Garage": garage,
    "Distance_to_City_Center": distance_to_city,
    "Price": price
})

# Save to CSV
df1.to_csv("house_prices.csv", index=False)

print("Dataset generated and saved as 'house_prices.csv'.")

#cleaning the dataset
# Load dataset
df1 = pd.read_csv("/content/house_prices.csv")

# Check for missing values
print("Missing values:\n", df1.isnull().sum())

# Check for duplicates
print("Duplicate entries:", df1.duplicated().sum())

# Remove duplicates if any exist
df1.drop_duplicates(inplace=True)

# Check for outliers using boxplots
import seaborn as sns
import matplotlib.pyplot as plt

sns.boxplot(x=df1["Price"])
plt.title("Outlier Detection - House Prices")
plt.show()

# Remove extreme outliers (beyond 1st & 99th percentile)
df1 = df1[df1["Price"].between(df1["Price"].quantile(0.01), df1["Price"].quantile(0.99))]

#Linear Regression Analysis
# dependent variable (Y) = price
#Independent variable (X) = Size_sqft, Bedrroms, Bathrooms, Garage, Distance_to_city_center

import statsmodels.api as sm

# Define dependent & independent variables
X = df1[["Size_sqft", "Bedrooms", "Bathrooms", "Garage", "Distance_to_City_Center"]]
y = df1["Price"]

# Add constant term for regression
X = sm.add_constant(X)

# Fit the model
model = sm.OLS(y, X).fit()

# Print summary
print(model.summary())

import scipy.stats as stats
stats.probplot(model.resid, plot=plt)
plt.title("Q-Q Plot of Residuals")
plt.show()

sns.scatterplot(x=model.fittedvalues, y=model.resid)
plt.axhline(y=0, color='r', linestyle='--')
plt.title("Residuals vs. Fitted Values")
plt.show()

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = pd.DataFrame({"Variable": X.columns, "VIF": [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]})
print(vif)

#ANOVA Analysis
from statsmodels.stats.anova import anova_lm
import statsmodels.formula.api as smf

# Fit the model using the formula API
# The formula should be 'dependent_variable ~ independent_variable1 + independent_variable2 + ...'
# In this case, 'Price ~ Size_sqft + Bedrooms + Bathrooms + Garage + Distance_to_City_Center'
model_formula = smf.ols('Price ~ Size_sqft + Bedrooms + Bathrooms + Garage + Distance_to_City_Center', data=df1)
model_fitted = model_formula.fit()

# Perform ANOVA
anova_results = anova_lm(model_fitted)
print(anova_results)

#Since sm.OLS() directly fits a model on a Pandas dataframe, anova_lm() can't extract design matrix information. The fix is to use ols() from statsmodels.formula.api, which allows ANOVA computations.

#Solution: Use Formula-Based Regression
import statsmodels.formula.api as smf
from statsmodels.stats.anova import anova_lm

# Load your HOUSE PRICES dataset (not salary data)
df = pd.read_csv("/content/house_prices.csv")

# Fit model with formula (using actual columns from house_prices.csv)
model_formula = smf.ols(
    "Price ~ Size_sqft + Bedrooms + Bathrooms + Garage + Distance_to_City_Center",
    data=df
).fit()

# Perform ANOVA
anova_results = anova_lm(model_formula, typ=2)  # Type 2 ANOVA (recommended)
print(anova_results)

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Compute VIF for each independent variable
X_vars = df1[["Size_sqft", "Bedrooms", "Bathrooms", "Garage", "Distance_to_City_Center"]]
X_vars = sm.add_constant(X_vars)  # Add constant
vif_data = pd.DataFrame()
vif_data["Feature"] = X_vars.columns
vif_data["VIF"] = [variance_inflation_factor(X_vars.values, i) for i in range(X_vars.shape[1])]

print(vif_data)

#2nd dataset for linear regression
np.random.seed(42)

n = 400
years_experience = np.random.randint(1, 30, size=n)
education = np.random.choice([0, 1], size=n, p=[0.3, 0.7])
certifications = np.random.randint(0, 10, size=n)
industry = np.random.choice([0, 1], size=n, p=[0.5, 0.5])
salary = years_experience * np.random.uniform(2000, 3000, size=n) + (education * 5000) + (certifications * 1000) + (industry * 7000)

df2 = pd.DataFrame({
    "Employee_ID": range(1, n+1),
    "Years_of_Experience": years_experience,
    "Education_Level": education,
    "Certifications": certifications,
    "Industry": industry,
    "Salary": salary
})

df2.to_csv("employee_salaries.csv", index=False)

print("Dataset generated and saved as 'employee_salaries.csv'.")

#Data Cleaning
df2 = pd.read_csv("/content/employee_salaries.csv")

# Handle missing values
df2.fillna(df2.median(), inplace=True)

# Remove duplicate records
df2.drop_duplicates(inplace=True)

# Remove outliers (beyond 1st & 99th percentile)
df2 = df2[df2["Salary"].between(df2["Salary"].quantile(0.01), df2["Salary"].quantile(0.99))]

#Linear Regression Analysis
X = df2[["Years_of_Experience", "Education_Level", "Certifications", "Industry"]]
y = df2["Salary"]
X = sm.add_constant(X)

model = sm.OLS(y, X).fit()
print(model.summary())

from statsmodels.stats.anova import anova_lm
import statsmodels.formula.api as smf

# Fit the model using the formula API
# The formula should be 'dependent_variable ~ independent_variable1 + independent_variable2 + ...'
model_formula = smf.ols('Salary ~ Years_of_Experience + Education_Level + Certifications + Industry', data=df2)
model_fitted = model_formula.fit()

# Perform ANOVA
anova_results = anova_lm(model_fitted, typ=2) # Use Type 2 ANOVA (recommended)
print(anova_results)

#Instead of sm.OLS(y, X), use ols() from statsmodels.formula.api:

import statsmodels.formula.api as smf
from statsmodels.stats.anova import anova_lm

# Convert to formula-based regression
model_formula = smf.ols("Salary ~ Years_of_Experience + Education_Level + Certifications + Industry", data=df2).fit()

# Perform ANOVA
anova_results = anova_lm(model_formula)
print(anova_results)

#Regression Assumptions Check
# Normality of residuals
residuals = model.resid
sns.histplot(residuals, kde=True)
plt.title("Residual Normality Check")
plt.show()

# Homoscedasticity check
plt.scatter(model.predict(X), residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.title("Homoscedasticity Check")
plt.show()

